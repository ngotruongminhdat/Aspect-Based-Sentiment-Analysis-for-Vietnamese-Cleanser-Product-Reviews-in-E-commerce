{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KL6ghpihDzdc",
        "outputId": "92e85337-c2e1-4c3c-a6f1-4631d0a6164b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input\n",
        "from keras.layers import Dense, Dropout, Concatenate\n",
        "from keras.layers import LSTM, Embedding, Bidirectional, GRU\n",
        "from keras.layers import SpatialDropout1D, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.initializers import Constant\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.losses import BinaryCrossentropy, CategoricalCrossentropy\n",
        "import pickle\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report\n",
        "import collections\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from gensim import models"
      ],
      "metadata": {
        "id": "sz3O0h2PD9bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = pd.read_csv('/content/drive/My Drive/ML/Test Thesis/data_train.csv')\n",
        "data_val = pd.read_csv('/content/drive/My Drive/ML/Test Thesis//data_val.csv')\n",
        "data_test = pd.read_csv('/content/drive/My Drive/ML/Test Thesis/data_test.csv')\n",
        "\n",
        "data_train.drop('Unnamed: 0', axis = 1, inplace = True)\n",
        "data_val.drop('Unnamed: 0', axis = 1, inplace = True)\n",
        "data_test.drop('Unnamed: 0', axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "f794cSjfEHVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PhoBERT"
      ],
      "metadata": {
        "id": "XpXCNBBpXRnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n",
        "model = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
        "\n",
        "def get_embedding(sentence):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=256)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        embedding = outputs[0][:, 0, :].squeeze().numpy()\n",
        "    return embedding\n",
        "\n",
        "embeddings = data_train['processed_content'].apply(get_embedding)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ertPPKf4jERB",
        "outputId": "77847c96-d253-4fd6-bacb-48babb1ae374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('/content/drive/MyDrive/Test Thesis/embeddings_train_PhoBert.npy', embeddings)"
      ],
      "metadata": {
        "id": "QXTf9j3unW7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
        "model = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
        "\n",
        "def get_embedding(sentence):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=256)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        embedding = outputs[0][:, 0, :].squeeze().numpy()\n",
        "    return embedding\n",
        "\n",
        "embeddings = data_val['processed_content'].apply(get_embedding)\n",
        "np.save('/content/drive/MyDrive/Test Thesis/embeddings_val_PhoBert.npy', embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0ksv25Xn7V1",
        "outputId": "2cfa361e-6db4-4203-b188-1a82befdf4ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
        "model = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
        "\n",
        "def get_embedding(sentence):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=256)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        embedding = outputs[0][:, 0, :].squeeze().numpy()\n",
        "    return embedding\n",
        "\n",
        "embeddings = data_test['processed_content'].apply(get_embedding)\n",
        "np.save('/content/drive/MyDrive/Test Thesis/embeddings_test_PhoBert.npy', embeddings)"
      ],
      "metadata": {
        "id": "8CoExW8_64A4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SimCSE"
      ],
      "metadata": {
        "id": "arqgoajw4xjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from pyvi.ViTokenizer import tokenize\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "df = {\n",
        "    'train': data_train,\n",
        "    'val': data_val,\n",
        "    'test': data_test\n",
        "}\n",
        "\n",
        "PhobertTokenizer = AutoTokenizer.from_pretrained(\"VoVanPhuc/sup-SimCSE-VietNamese-phobert-base\")\n",
        "model = AutoModel.from_pretrained(\"VoVanPhuc/sup-SimCSE-VietNamese-phobert-base\")\n",
        "\n",
        "def process_batch(model, tokenizer, sentences):\n",
        "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
        "    return outputs.cpu().numpy()\n",
        "batch_size = 200\n",
        "embeddings = {}\n",
        "\n",
        "for state in tqdm(['train','val','test']):\n",
        "    sentences = df[state].processed_content.values\n",
        "    state_embeddings = []\n",
        "    for i in tqdm(range(0, len(sentences), batch_size)):\n",
        "        batch_sentences = sentences[i:i+batch_size]\n",
        "        batch_sentences = [tokenize(sentence) for sentence in batch_sentences]\n",
        "        batch_embeddings = process_batch(model, PhobertTokenizer, batch_sentences)\n",
        "        state_embeddings.append(batch_embeddings)\n",
        "    embeddings[state] = np.concatenate(state_embeddings, axis=0)\n",
        "\n",
        "np.save('/content/drive/MyDrive/Thesis/embeddings_train.npy', embeddings['train'])\n",
        "np.save('/content/drive/MyDrive/Thesis/embeddings_train.npy', embeddings['val'])\n",
        "np.save('/content/drive/MyDrive/Thesis/embeddings_train.npy', embeddings['test'])"
      ],
      "metadata": {
        "id": "-Eil1tpdXT9u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}